[
["index.html", "An Introduction to Statistical Learning What this is about", " An Introduction to Statistical Learning with (Tidy) Applications in R Benjamin Soltoff 2018-07-24 What this is about This book is intended to serve as a companion to An Introduction to Statistical Learning with Applications in R (ISLR) and reimplement the R programs using primarily tidyverse packages and a tidy design philosophy. It is not intended to stand on its own or review the statistical learning concepts from ISLR. It is not intended to teach R programming or computational problem solving in general. For that, I recommend any number of online resources including Hadley Wickham’s R for Data Science and the materials from my graduate course on Computing for the Social Sciences. The original ISLR book is copyrighted by Springer. The original contribution in work is licensed under the CC BY-NC 4.0 Creative Commons License. "],
["intro.html", "1 Introduction", " 1 Introduction No labs included in this chapter. "],
["stat-learn.html", "2 What is statistical learning? 2.1 What is statistical learning? 2.2 Assessing model accuracy 2.3 Lab: Introduction to R 2.4 Install and load tidyverse packages 2.5 Session information", " 2 What is statistical learning? 2.1 What is statistical learning? 2.2 Assessing model accuracy 2.3 Lab: Introduction to R 2.3.1 Basic commands x &lt;- c(1, 3, 2, 5) x ## [1] 1 3 2 5 Use the assignment operator &lt;- to create new objects in R. Objects can be replaced (or overridden) by creating a new object with the same name. x &lt;- c(1, 6, 2) y &lt;- c(1, 4, 3) x ## [1] 1 6 2 y ## [1] 1 4 3 R performs simple mathematical calculations. For instance, to add numbers use the + notation. This will add the first value of x to the first value of y, and so on. R uses vector recycling if you try to combine vectors that are not the same length, so use length() to confirm that x and y contain the same number of values. length(x) ## [1] 3 length(y) ## [1] 3 x + y ## [1] 2 10 5 Many tidyverse functions will not implicitly perform vector recycling. If you need values or vectors repeated, you will need to explicitly repeat the values first and then run the function. ls() lists the names of all the objects currently in your working environment. ls() ## [1] &quot;x&quot; &quot;y&quot; If you are using RStudio, you can see this list in the upper-right panel of the IDE: RStudio IDE with environment highlighted Most tidyverse functions assume your data is stored in a data frame. A data frame is a spreadsheet style data object which stores values in columns and rows. A tidy data frame adheres to three basic principles: Each variable must have its own column Each observation must have its own row Each value must have its own cell Tibbles are a special type of data frame which work nicely with tidyverse packages and RStudio. To create a tibble, we first need to load the tibble package. Packages in R contain additional functions which build new features onto the base R software. Packages are loaded using the library() function, at which point all the functions in the library are now directly accessible. Use install.packages() if you do not yet have this package installed: install.packages(&quot;tibble&quot;) library(tibble) To create a tibble, we use the tibble() function: tibble( x = 1:5, y = 1, z = x ^ 2 + y ) ## # A tibble: 5 x 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 ## 2 2 1 5 ## 3 3 1 10 ## 4 4 1 17 ## 5 5 1 26 Each column of a tibble is defined as a vector, and columns can be created either by individual vectors or by recycling inputs of length 1. You can also create variables that are derived from already created columns (hence z). 2.3.2 Graphics ggplot2 is the tidyverse preferred package for generating graphics in R. It is structured on the layered grammar of graphics and provides a consistent syntax for creating both basic and advanced statistical graphs. See the data visualization cheat sheet for a summary of the core graphing functions in ggplot2. For instance, to create a basic scatterplot: # create simulated data scatter_data &lt;- tibble( x = rnorm(100), y = rnorm(100) ) library(ggplot2) # load ggplot2 # generate scatterplot ggplot(data = scatter_data, mapping = aes(x = x, y = y)) + geom_point() ggplot2 builds graphs in layers, so additional components are added using the + notation. To add labels to this graph, use the labs() function. ggplot(data = scatter_data, mapping = aes(x = x, y = y)) + geom_point() + labs(title = &quot;This is a plot of X vs Y&quot;, x = &quot;This is the x-axis&quot;, y = &quot;This is the y-axis&quot;) To export ggplot() objects, use ggsave(): x &lt;- ggplot(data = scatter_data, mapping = aes(x = x, y = y)) + geom_point() + labs(title = &quot;This is a plot of X vs Y&quot;, x = &quot;This is the x-axis&quot;, y = &quot;This is the y-axis&quot;) ggsave(filename = &quot;scatterplot.pdf&quot;, plot = x) ggplot2 is an excellent package for creating static two-dimensional graphs. For interactive or three-dimensional graphs, consider plot.ly or highcharter. 2.3.3 Indexing data Sometimes you want to examine only a portion of a tibble. Beyond the base R [ and [[ subsetting approaches, dplyr provides two core functions for subsetting a data frame. Consider the following tibble: df &lt;- tibble( x = 1:5, y = 1, z = x ^ 2 + y ) df ## # A tibble: 5 x 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 ## 2 2 1 5 ## 3 3 1 10 ## 4 4 1 17 ## 5 5 1 26 To subset specific rows, use filter(): library(dplyr) ## Warning: package &#39;dplyr&#39; was built under R version 3.5.1 ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union filter(.data = df, x &gt; 3) ## # A tibble: 2 x 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 1 17 ## 2 5 1 26 filter(.data = df, z &lt; 5) ## # A tibble: 1 x 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 To subset specific columns, use select(): select(.data = df, x, y) ## # A tibble: 5 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 1 ## 2 2 1 ## 3 3 1 ## 4 4 1 ## 5 5 1 select(.data = df, -y) ## # A tibble: 5 x 2 ## x z ## &lt;int&gt; &lt;dbl&gt; ## 1 1 2 ## 2 2 5 ## 3 3 10 ## 4 4 17 ## 5 5 26 2.3.4 Loading data To import rectangular data files like .csv or .tsv, use read_csv() or read_tsv() from the readr package: library(readr) Auto &lt;- read_csv(&quot;data/auto.csv&quot;) ## Parsed with column specification: ## cols( ## mpg = col_double(), ## cylinders = col_integer(), ## displacement = col_double(), ## horsepower = col_integer(), ## weight = col_double(), ## acceleration = col_double(), ## year = col_integer(), ## origin = col_integer(), ## name = col_character() ## ) Auto ## # A tibble: 392 x 9 ## mpg cylinders displacement horsepower weight acceleration year origin ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 18 8 307 130 3504 12 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11 70 1 ## 4 16 8 304 150 3433 12 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10 70 1 ## # ... with 386 more rows, and 1 more variable: name &lt;chr&gt; read_() functions automatically decode each column type, a header row (if available), and import the data quickly and efficiently. Generally these guesses for column type are accurate, though they can always be manually defined. To import other file types, consider these packages: haven - SAS, SPSS, and Stata readxl - Excel googledrive - Google Sheets 2.3.5 Additional graphical and numerical summaries Variable names are passed to ggplot() using the aes() function. ggplot(data = Auto, mapping = aes(x = cylinders, y = mpg)) + geom_point() Since cylinders is essentially a categorical variable (not enough unique values to be considered continuous), we could store it as a qualitative variable using as.factor() and then visualize this data using a boxplot. To convert a column in-place, we use mutate() from the dplyr package: # convert cylinders to a factor variable Auto &lt;- mutate(.data = Auto, cylinders = as.factor(cylinders)) Auto ## # A tibble: 392 x 9 ## mpg cylinders displacement horsepower weight acceleration year origin ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 18 8 307 130 3504 12 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11 70 1 ## 4 16 8 304 150 3433 12 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10 70 1 ## # ... with 386 more rows, and 1 more variable: name &lt;chr&gt; ggplot(data = Auto, mapping = aes(x = cylinders, y = mpg)) + geom_boxplot() The visual appearance of the boxplot can be customized using either additional arguments to geom_boxplot() or adding additional components: ggplot(data = Auto, mapping = aes(x = cylinders, y = mpg)) + geom_boxplot(color = &quot;red&quot;) ggplot(data = Auto, mapping = aes(x = cylinders, y = mpg)) + geom_boxplot(color = &quot;red&quot;) + labs(x = &quot;Number of cylinders&quot;, y = &quot;MPG&quot;) To create a scatterplot matrix, use ggpairs() from the GGally package: library(GGally) ## ## Attaching package: &#39;GGally&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## nasa ggpairs(data = select(.data = Auto, -name)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We need to exclude the name column because it is just an ID column - there is nothing informative in this column to create a scatterplot matrix. We could also write this code using the pipe operator %&gt;% to first subset the tibble, then create the scatterplot matrix: select(.data = Auto, -name) %&gt;% ggpairs() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Piped operations are a powerful tool in the tidyverse to write human-readable code that clearly defines each step of a multi-operation chunk of code. 2.4 Install and load tidyverse packages The easiest method to install and load tidyverse packages is to install tidyverse. This package automatically downloads and installs the complete tidyverse. When you load the package with library(tidyverse), it will automatically load the core tidyverse and make it available in your current R session. The core tidyverse is a set of packages you are likely to use in everyday data analyses. All other tidyverse packages can be loaded directly using library(). install.packages(&quot;tidyverse&quot;) library(tidyverse) ## ── Attaching packages ───────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ## ✔ purrr 0.2.5 ✔ forcats 0.3.0 ## ── Conflicts ──────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 2.5 Session information devtools::session_info() ## Session info ------------------------------------------------------------- ## setting value ## version R version 3.5.0 (2018-04-23) ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz America/Chicago ## date 2018-07-24 ## Packages ----------------------------------------------------------------- ## package * version date source ## backports 1.1.2 2017-12-13 CRAN (R 3.5.0) ## base * 3.5.0 2018-04-24 local ## bookdown 0.7 2018-02-18 CRAN (R 3.5.0) ## compiler 3.5.0 2018-04-24 local ## datasets * 3.5.0 2018-04-24 local ## devtools 1.13.5 2018-02-18 CRAN (R 3.5.0) ## digest 0.6.15 2018-01-28 CRAN (R 3.5.0) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.5.0) ## graphics * 3.5.0 2018-04-24 local ## grDevices * 3.5.0 2018-04-24 local ## htmltools 0.3.6 2017-04-28 CRAN (R 3.5.0) ## knitr 1.20 2018-02-20 CRAN (R 3.5.0) ## magrittr 1.5 2014-11-22 CRAN (R 3.5.0) ## memoise 1.1.0 2017-04-21 CRAN (R 3.5.0) ## methods * 3.5.0 2018-04-24 local ## Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) ## rmarkdown 1.9 2018-03-01 CRAN (R 3.5.0) ## rprojroot 1.3-2 2018-01-03 CRAN (R 3.5.0) ## stats * 3.5.0 2018-04-24 local ## stringi 1.2.2 2018-05-02 CRAN (R 3.5.0) ## stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) ## tools 3.5.0 2018-04-24 local ## utils * 3.5.0 2018-04-24 local ## withr 2.1.2 2018-03-15 CRAN (R 3.5.0) ## xfun 0.1 2018-01-22 CRAN (R 3.5.0) ## yaml 2.1.19 2018-05-01 CRAN (R 3.5.0) "],
["lin-reg.html", "3 Linear regression 3.1 Simple linear regression 3.2 Multiple linear regression 3.3 Other considerations in the regression model 3.4 The marketing plan 3.5 Comparison of linear regression with \\(K\\)-nearest neighbors 3.6 Lab: Linear regression 3.7 Session information", " 3 Linear regression library(tidyverse) ## ── Attaching packages ───────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.0.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.6 ## ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.1 ## ── Conflicts ──────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 3.1 Simple linear regression 3.2 Multiple linear regression 3.3 Other considerations in the regression model 3.4 The marketing plan 3.5 Comparison of linear regression with \\(K\\)-nearest neighbors 3.6 Lab: Linear regression 3.6.1 Libraries ISLR contains several datasets associated with An Introduction to Statistical Learning. MASS also contains a large number of data sets and functions, one of which is used in this lab. However MASS also contains a function called filter(), which would conflict with filter() from dplyr. To avoid this conflict, we can use data() to directly access the data frame without loading the entire MASS package. We then convert this data frame to a tibble using as_tibble() to ensure proper formatting and appearance. library(ISLR) data(Boston, package = &quot;MASS&quot;) Boston &lt;- as_tibble(Boston) Boston ## # A tibble: 506 x 14 ## crim zn indus chas nox rm age dis rad tax ptratio ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00632 18 2.31 0 0.538 6.58 65.2 4.09 1 296 15.3 ## 2 0.0273 0 7.07 0 0.469 6.42 78.9 4.97 2 242 17.8 ## 3 0.0273 0 7.07 0 0.469 7.18 61.1 4.97 2 242 17.8 ## 4 0.0324 0 2.18 0 0.458 7.00 45.8 6.06 3 222 18.7 ## 5 0.0690 0 2.18 0 0.458 7.15 54.2 6.06 3 222 18.7 ## 6 0.0298 0 2.18 0 0.458 6.43 58.7 6.06 3 222 18.7 ## # ... with 500 more rows, and 3 more variables: black &lt;dbl&gt;, lstat &lt;dbl&gt;, ## # medv &lt;dbl&gt; 3.6.2 Simple linear regression Boston is a dataset of neighborhood statistics for 506 neighborhoods around Boston, Massachusetts. We will attempt to predict medv (median value of owner-occupied homes in $1000s) using 13 predictors such as rm (average number of rooms per house), age (average age of houses), and lstat (percent of households with low socioeconomic status). To estimate a linear regression model in R, use the core lm() function. The syntax is lm(response ~ predictors, data = dataframe), like this: lm_fit &lt;- lm(medv ~ lstat, data = Boston) To view a summary of the linear regression model, the base R approach uses summary(): summary(lm_fit) ## ## Call: ## lm(formula = medv ~ lstat, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.17 -3.99 -1.32 2.03 24.50 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.5538 0.5626 61.4 &lt;2e-16 *** ## lstat -0.9500 0.0387 -24.5 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.22 on 504 degrees of freedom ## Multiple R-squared: 0.544, Adjusted R-squared: 0.543 ## F-statistic: 602 on 1 and 504 DF, p-value: &lt;2e-16 This prints some output to the console, including information about the coefficients, standard errors, and overall model statistics. This approach is not tidy because the object containing all this information is not a data frame. Instead, we can use the broom package to summarize and extract key information about statistical models in tidy tibbles. To view summaries about each component of the model (in this case, the regression coefficients), use tidy(): library(broom) tidy(lm_fit) ## term estimate std.error statistic p.value ## 1 (Intercept) 34.55 0.5626 61.4 3.74e-236 ## 2 lstat -0.95 0.0387 -24.5 5.08e-88 glance() returns a tibble with one row of goodness of fit measures and related statistics. glance(lm_fit) ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## 1 0.544 0.543 6.22 602 5.08e-88 2 -1641 3289 3302 ## deviance df.residual ## 1 19472 504 augment() returns a data frame with one row per observation from the original dataset and adds information such as fitted values, residuals, etc. augment(lm_fit) %&gt;% as_tibble() ## # A tibble: 506 x 9 ## medv lstat .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 24 4.98 29.8 0.406 -5.82 0.00426 6.22 0.00189 -0.939 ## 2 21.6 9.14 25.9 0.308 -4.27 0.00246 6.22 0.000582 -0.688 ## 3 34.7 4.03 30.7 0.433 3.97 0.00486 6.22 0.00100 0.641 ## 4 33.4 2.94 31.8 0.467 1.64 0.00564 6.22 0.000198 0.264 ## 5 36.2 5.33 29.5 0.396 6.71 0.00406 6.21 0.00238 1.08 ## 6 28.7 5.21 29.6 0.399 -0.904 0.00413 6.22 0.0000440 -0.146 ## # ... with 500 more rows To visualize the linear regression model, we can use geom_smooth(): ggplot(data = Boston, mapping = aes(x = lstat, y = medv)) + geom_point() + geom_smooth(method = &quot;lm&quot;) This not only draws the best fit line but also generates a 95% confidence interval. Visualizing diagnostic plots can also be done using augment() and ggplot(). augment() automatically calculates statistics such as residuals and leverage statistics. augment(lm_fit) %&gt;% ggplot(mapping = aes(x = .fitted, y = .resid)) + geom_point() + labs(x = &quot;Fitted value&quot;, y = &quot;Residual&quot;) augment(lm_fit) %&gt;% ggplot(mapping = aes(x = .fitted, y = .std.resid)) + geom_point() + labs(x = &quot;Fitted value&quot;, y = &quot;Standardized residual&quot;) augment(lm_fit) %&gt;% ggplot(mapping = aes(x = .hat)) + geom_histogram() + labs(x = &quot;Hat value&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 3.6.3 Multiple linear regression Again we use lm(), now specifying multiple predictors using x1 + x2 + x3 notation. lm_fit &lt;- lm(medv ~ lstat + age, data = Boston) tidy(lm_fit) ## term estimate std.error statistic p.value ## 1 (Intercept) 33.2228 0.7308 45.46 2.94e-180 ## 2 lstat -1.0321 0.0482 -21.42 8.42e-73 ## 3 age 0.0345 0.0122 2.83 4.91e-03 glance(lm_fit) ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## 1 0.551 0.549 6.17 309 2.98e-88 3 -1638 3283 3300 ## deviance df.residual ## 1 19168 503 To automatically regress on all of the available predictors: lm_fit &lt;- lm(medv ~ ., data = Boston) tidy(lm_fit) ## term estimate std.error statistic p.value ## 1 (Intercept) 3.65e+01 5.10346 7.1441 3.28e-12 ## 2 crim -1.08e-01 0.03286 -3.2865 1.09e-03 ## 3 zn 4.64e-02 0.01373 3.3816 7.78e-04 ## 4 indus 2.06e-02 0.06150 0.3343 7.38e-01 ## 5 chas 2.69e+00 0.86158 3.1184 1.93e-03 ## 6 nox -1.78e+01 3.81974 -4.6513 4.25e-06 ## 7 rm 3.81e+00 0.41793 9.1161 1.98e-18 ## 8 age 6.92e-04 0.01321 0.0524 9.58e-01 ## 9 dis -1.48e+00 0.19945 -7.3980 6.01e-13 ## 10 rad 3.06e-01 0.06635 4.6129 5.07e-06 ## 11 tax -1.23e-02 0.00376 -3.2800 1.11e-03 ## 12 ptratio -9.53e-01 0.13083 -7.2825 1.31e-12 ## 13 black 9.31e-03 0.00269 3.4668 5.73e-04 ## 14 lstat -5.25e-01 0.05072 -10.3471 7.78e-23 glance(lm_fit) ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## 1 0.741 0.734 4.75 108 6.72e-135 14 -1499 3028 3091 ## deviance df.residual ## 1 11079 492 Use vif() from the car package to calculate variance inflation factors (VIFs). library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some vif(lm_fit) ## crim zn indus chas nox rm age dis rad ## 1.79 2.30 3.99 1.07 4.39 1.93 3.10 3.96 7.48 ## tax ptratio black lstat ## 9.01 1.80 1.35 2.94 3.6.4 Interaction terms Use the syntax lstat * age to simultaneously include lstat, age, and the interaction term lstat x age. Never omit constitutive terms. lm(medv ~ lstat * age, data = Boston) %&gt;% tidy() ## term estimate std.error statistic p.value ## 1 (Intercept) 36.088536 1.46984 24.5528 4.91e-88 ## 2 lstat -1.392117 0.16746 -8.3134 8.78e-16 ## 3 age -0.000721 0.01988 -0.0363 9.71e-01 ## 4 lstat:age 0.004156 0.00185 2.2443 2.52e-02 3.6.5 Non-linear transformations of the predictors Add non-linear transformations of predictors using I(x ^ 2) notation. So to add a second-order polynomial term: lm_fit2 &lt;- lm(medv ~ lstat + I(lstat ^ 2), data = Boston) tidy(lm_fit2) ## term estimate std.error statistic p.value ## 1 (Intercept) 42.8620 0.87208 49.1 3.50e-194 ## 2 lstat -2.3328 0.12380 -18.8 2.55e-60 ## 3 I(lstat^2) 0.0435 0.00375 11.6 7.63e-28 Third and higher order terms are best implemented using poly(), which allows you to specify the highest-order term and all lower-order terms are automatically created. lm_fit5 &lt;- lm(medv ~ poly(x = lstat, degree = 5), data = Boston) tidy(lm_fit5) ## term estimate std.error statistic p.value ## 1 (Intercept) 22.5 0.232 97.20 0.00e+00 ## 2 poly(x = lstat, degree = 5)1 -152.5 5.215 -29.24 2.69e-110 ## 3 poly(x = lstat, degree = 5)2 64.2 5.215 12.32 1.25e-30 ## 4 poly(x = lstat, degree = 5)3 -27.1 5.215 -5.19 3.10e-07 ## 5 poly(x = lstat, degree = 5)4 25.5 5.215 4.88 1.42e-06 ## 6 poly(x = lstat, degree = 5)5 -19.3 5.215 -3.69 2.47e-04 3.6.6 Qualitative predictors Examine the CarSeats data in the ISLR library. as_tibble(Carseats) ## # A tibble: 400 x 11 ## Sales CompPrice Income Advertising Population Price ShelveLoc Age ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 9.5 138 73 11 276 120 Bad 42 ## 2 11.2 111 48 16 260 83 Good 65 ## 3 10.1 113 35 10 269 80 Medium 59 ## 4 7.4 117 100 4 466 97 Medium 55 ## 5 4.15 141 64 3 340 128 Bad 38 ## 6 10.8 124 113 13 501 72 Bad 78 ## # ... with 394 more rows, and 3 more variables: Education &lt;dbl&gt;, ## # Urban &lt;fct&gt;, US &lt;fct&gt; While some of the variables are continuous, others such as ShelveLoc (quality of shelf location) are qualitative. ShelveLoc takes on three possible values: Bad, Medium, and Good. Given qualitative variables, R automatically converts them to a series of dummy variables with 0/1 coding: lm_fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats) tidy(lm_fit) ## term estimate std.error statistic p.value ## 1 (Intercept) 6.575565 1.008747 6.519 2.22e-10 ## 2 CompPrice 0.092937 0.004118 22.567 1.64e-72 ## 3 Income 0.010894 0.002604 4.183 3.57e-05 ## 4 Advertising 0.070246 0.022609 3.107 2.03e-03 ## 5 Population 0.000159 0.000368 0.433 6.65e-01 ## 6 Price -0.100806 0.007440 -13.549 1.74e-34 ## 7 ShelveLocGood 4.848676 0.152838 31.724 1.38e-109 ## 8 ShelveLocMedium 1.953262 0.125768 15.531 1.34e-42 ## 9 Age -0.057947 0.015951 -3.633 3.18e-04 ## 10 Education -0.020852 0.019613 -1.063 2.88e-01 ## 11 UrbanYes 0.140160 0.112402 1.247 2.13e-01 ## 12 USYes -0.157557 0.148923 -1.058 2.91e-01 ## 13 Income:Advertising 0.000751 0.000278 2.698 7.29e-03 ## 14 Price:Age 0.000107 0.000133 0.801 4.24e-01 3.6.7 Writing functions 3.7 Session information devtools::session_info() ## Session info ------------------------------------------------------------- ## setting value ## version R version 3.5.0 (2018-04-23) ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz America/Chicago ## date 2018-07-24 ## Packages ----------------------------------------------------------------- ## package * version date source ## backports 1.1.2 2017-12-13 CRAN (R 3.5.0) ## base * 3.5.0 2018-04-24 local ## bookdown 0.7 2018-02-18 CRAN (R 3.5.0) ## compiler 3.5.0 2018-04-24 local ## datasets * 3.5.0 2018-04-24 local ## devtools 1.13.5 2018-02-18 CRAN (R 3.5.0) ## digest 0.6.15 2018-01-28 CRAN (R 3.5.0) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.5.0) ## graphics * 3.5.0 2018-04-24 local ## grDevices * 3.5.0 2018-04-24 local ## htmltools 0.3.6 2017-04-28 CRAN (R 3.5.0) ## knitr 1.20 2018-02-20 CRAN (R 3.5.0) ## magrittr 1.5 2014-11-22 CRAN (R 3.5.0) ## memoise 1.1.0 2017-04-21 CRAN (R 3.5.0) ## methods * 3.5.0 2018-04-24 local ## Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) ## rmarkdown 1.9 2018-03-01 CRAN (R 3.5.0) ## rprojroot 1.3-2 2018-01-03 CRAN (R 3.5.0) ## stats * 3.5.0 2018-04-24 local ## stringi 1.2.2 2018-05-02 CRAN (R 3.5.0) ## stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) ## tools 3.5.0 2018-04-24 local ## utils * 3.5.0 2018-04-24 local ## withr 2.1.2 2018-03-15 CRAN (R 3.5.0) ## xfun 0.1 2018-01-22 CRAN (R 3.5.0) ## yaml 2.1.19 2018-05-01 CRAN (R 3.5.0) "],
["classification.html", "4 Classification 4.1 An overview of classification 4.2 Why not linear regression? 4.3 Logistic regression 4.4 Linear discriminant analysis 4.5 A comparison of classification methods 4.6 Lab: Logistic regression, LDA, QDA, and KNN 4.7 Session information", " 4 Classification library(tidyverse) ## ── Attaching packages ───────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.0.0 ✔ purrr 0.2.5 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.6 ## ✔ tidyr 0.8.1 ✔ stringr 1.3.1 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.1 ## ── Conflicts ──────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 4.1 An overview of classification 4.2 Why not linear regression? 4.3 Logistic regression 4.4 Linear discriminant analysis 4.5 A comparison of classification methods 4.6 Lab: Logistic regression, LDA, QDA, and KNN 4.6.1 The stock market data Load the SMarket data from ISLR. library(ISLR) Smarket &lt;- as_tibble(Smarket) Smarket ## # A tibble: 1,250 x 9 ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 2001 0.381 -0.192 -2.62 -1.06 5.01 1.19 0.959 Up ## 2 2001 0.959 0.381 -0.192 -2.62 -1.06 1.30 1.03 Up ## 3 2001 1.03 0.959 0.381 -0.192 -2.62 1.41 -0.623 Down ## 4 2001 -0.623 1.03 0.959 0.381 -0.192 1.28 0.614 Up ## 5 2001 0.614 -0.623 1.03 0.959 0.381 1.21 0.213 Up ## 6 2001 0.213 0.614 -0.623 1.03 0.959 1.35 1.39 Up ## # ... with 1,244 more rows Brief overview of the data using a scatterplot matrix: library(GGally) ## ## Attaching package: &#39;GGally&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## nasa ggpairs(Smarket) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Volume over time: Smarket %&gt;% mutate(id = row_number()) %&gt;% ggplot(mapping = aes(x = id, y = Volume)) + geom_line() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; 4.6.2 Logistic regression Logistic regression is a type of generalized linearmo del (GLM), a class of models for fitting regression lines to many types of response variables. glm() is the base function in R for estimating these models. The syntax is the same as lm() except we also pass the argument family = binomial to run the logistic regression form of GLM: glm_fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket, family = binomial) We can again use broom to summarize the output of glm(): library(broom) tidy(glm_fit) ## term estimate std.error statistic p.value ## 1 (Intercept) -0.12600 0.2407 -0.523 0.601 ## 2 Lag1 -0.07307 0.0502 -1.457 0.145 ## 3 Lag2 -0.04230 0.0501 -0.845 0.398 ## 4 Lag3 0.01109 0.0499 0.222 0.824 ## 5 Lag4 0.00936 0.0500 0.187 0.851 ## 6 Lag5 0.01031 0.0495 0.208 0.835 ## 7 Volume 0.13544 0.1584 0.855 0.392 glance(glm_fit) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 1731 1249 -864 1742 1778 1728 1243 To extract predicted probabilities for each observation (that is, in the form \\(P(Y = 1|X)\\)), we use augment() with the argument type.predict = &quot;response&quot;; if we omit that argument, the predicted values are generated in the log-odds form. augment(glm_fit, type.predict = &quot;response&quot;) %&gt;% as_tibble() ## # A tibble: 1,250 x 14 ## Direction Lag1 Lag2 Lag3 Lag4 Lag5 Volume .fitted .se.fit ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Up 0.381 -0.192 -2.62 -1.06 5.01 1.19 0.507 0.0732 ## 2 Up 0.959 0.381 -0.192 -2.62 -1.06 1.30 0.481 0.0415 ## 3 Down 1.03 0.959 0.381 -0.192 -2.62 1.41 0.481 0.0401 ## 4 Up -0.623 1.03 0.959 0.381 -0.192 1.28 0.515 0.0253 ## 5 Up 0.614 -0.623 1.03 0.959 0.381 1.21 0.511 0.0275 ## 6 Up 0.213 0.614 -0.623 1.03 0.959 1.35 0.507 0.0255 ## # ... with 1,244 more rows, and 5 more variables: .resid &lt;dbl&gt;, ## # .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; To convert these predicted probabilities to actual predictions using a \\(.5\\) threshold, we create a new column using mutate() which checks the .fitted value and returns Up if the probability is greater than or equal to \\(.5\\) and Down if the probability is less than \\(.5\\). augment(glm_fit, type.predict = &quot;response&quot;) %&gt;% as_tibble() %&gt;% mutate(.predict = ifelse(.fitted &gt;= .5, &quot;Up&quot;, &quot;Down&quot;)) ## # A tibble: 1,250 x 15 ## Direction Lag1 Lag2 Lag3 Lag4 Lag5 Volume .fitted .se.fit ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Up 0.381 -0.192 -2.62 -1.06 5.01 1.19 0.507 0.0732 ## 2 Up 0.959 0.381 -0.192 -2.62 -1.06 1.30 0.481 0.0415 ## 3 Down 1.03 0.959 0.381 -0.192 -2.62 1.41 0.481 0.0401 ## 4 Up -0.623 1.03 0.959 0.381 -0.192 1.28 0.515 0.0253 ## 5 Up 0.614 -0.623 1.03 0.959 0.381 1.21 0.511 0.0275 ## 6 Up 0.213 0.614 -0.623 1.03 0.959 1.35 0.507 0.0255 ## # ... with 1,244 more rows, and 6 more variables: .resid &lt;dbl&gt;, ## # .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;, ## # .predict &lt;chr&gt; We can create a confusion matrix by first counting the number Up/Down, Up/Up, Down/Up, and Down/Down pairs of actual and predicted outcomes, then using spread() from tidyr to cast the data frame into a wide format. augment(glm_fit, type.predict = &quot;response&quot;) %&gt;% as_tibble() %&gt;% mutate(.predict = ifelse(.fitted &gt;= .5, &quot;Up&quot;, &quot;Down&quot;)) %&gt;% count(Direction, .predict) %&gt;% spread(Direction, n) ## # A tibble: 2 x 3 ## .predict Down Up ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Down 145 141 ## 2 Up 457 507 Alternatively (and I think a bit more easily), the ISLR solution based on table() also works reasonably well: augment(glm_fit, type.predict = &quot;response&quot;) %&gt;% as_tibble() %&gt;% mutate(.predict = ifelse(.fitted &gt;= .5, &quot;Up&quot;, &quot;Down&quot;)) %&gt;% with(table(.predict, Direction)) ## Direction ## .predict Down Up ## Down 145 141 ## Up 457 507 with() allows us to directly refer to the column names without any additional notation. To calculate the predictive accuracy of the model, use mean(): augment(glm_fit, type.predict = &quot;response&quot;) %&gt;% as_tibble() %&gt;% mutate(.predict = ifelse(.fitted &gt;= .5, &quot;Up&quot;, &quot;Down&quot;)) %&gt;% with(mean(Direction != .predict)) ## [1] 0.478 This is the training error rate (portion of observations where the actual outcome does not match the predicted outcome). To calculate the test error rate, we hold back a portion of the data to evaluate the model’s effectiveness. Let’s split the data into years 2001-04 and 2005: Smarket_0104 &lt;- filter(Smarket, Year &lt; 2005) Smarket_05 &lt;- filter(Smarket, Year == 2005) Smarket_0104 ## # A tibble: 998 x 9 ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 2001 0.381 -0.192 -2.62 -1.06 5.01 1.19 0.959 Up ## 2 2001 0.959 0.381 -0.192 -2.62 -1.06 1.30 1.03 Up ## 3 2001 1.03 0.959 0.381 -0.192 -2.62 1.41 -0.623 Down ## 4 2001 -0.623 1.03 0.959 0.381 -0.192 1.28 0.614 Up ## 5 2001 0.614 -0.623 1.03 0.959 0.381 1.21 0.213 Up ## 6 2001 0.213 0.614 -0.623 1.03 0.959 1.35 1.39 Up ## # ... with 992 more rows Smarket_05 ## # A tibble: 252 x 9 ## Year Lag1 Lag2 Lag3 Lag4 Lag5 Volume Today Direction ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 2005 -0.134 0.008 -0.007 0.715 -0.431 0.787 -0.812 Down ## 2 2005 -0.812 -0.134 0.008 -0.007 0.715 1.51 -1.17 Down ## 3 2005 -1.17 -0.812 -0.134 0.008 -0.007 1.72 -0.363 Down ## 4 2005 -0.363 -1.17 -0.812 -0.134 0.008 1.74 0.351 Up ## 5 2005 0.351 -0.363 -1.17 -0.812 -0.134 1.57 -0.143 Down ## 6 2005 -0.143 0.351 -0.363 -1.17 -0.812 1.48 0.342 Up ## # ... with 246 more rows Let’s now train the model using the 2001-04 data: glm_fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Smarket_0104, family = binomial) And evaluate it using the 2005 data. The difference from before is we specify newdata = Smarket_05 to tell augment() to generate predicted values for the held-out 2005: augment(glm_fit, newdata = Smarket_05, type.predict = &quot;response&quot;) %&gt;% as_tibble() %&gt;% mutate(.predict = ifelse(.fitted &gt;= .5, &quot;Up&quot;, &quot;Down&quot;)) %&gt;% with(mean(Direction != .predict)) ## [1] 0.52 4.6.3 Linear discriminant analysis No broom implementation. Need to figure out how to proceed. 4.6.4 Quadratic discriminant analysis No broom implementation. Need to figure out how to proceed. 4.6.5 \\(K\\)-nearest neighbors Perform KNN using the knn() function in the class package. Unlike the past functions, we need to explicitly separate the predictors from the response variables. knn() requires four arguments: train - a data frame containing the predictors for the training data test - a data frame containing the predictors for the test data cl - a vector containing the class labels (i.e. outcomes) for the training observations k - the number of nearest neighbors to be used by the classifier We use select() to create the appropriate data frames for 1 and 2. Smarket_0104_x &lt;- select(Smarket_0104, -Direction) Smarket_05_x &lt;- select(Smarket_05, -Direction) Then we pass these data frames to knn(). To ensure reproducibility, we set the random seed before applying this function. set.seed(1234) library(class) knn_pred &lt;- knn(train = Smarket_0104_x, test = Smarket_05_x, cl = Smarket_0104$Direction, k = 1) knn_pred ## [1] Down Down Down Up Up Down Down Up Down Up Up Down Down Down ## [15] Up Down Up Up Up Up Up Up Down Up Down Up Down Up ## [29] Up Down Up Up Down Up Down Up Up Up Down Up Down Up ## [43] Up Up Down Down Down Down Up Up Down Up Down Down Down Up ## [57] Up Up Down Up Down Up Up Up Up Up Down Down Up Down ## [71] Down Down Up Up Down Up Down Up Down Up Down Up Up Down ## [85] Up Up Down Up Down Up Down Down Up Up Up Up Down Up ## [99] Up Up Up Up Down Up Down Down Up Down Down Up Down Up ## [113] Up Up Up Up Down Down Down Down Down Down Up Up Up Down ## [127] Up Down Up Up Up Up Up Up Up Down Up Up Down Up ## [141] Down Up Up Up Down Down Up Down Down Down Down Up Up Up ## [155] Down Down Down Down Up Up Up Down Down Down Down Up Up Up ## [169] Down Down Up Up Down Up Down Down Up Down Up Down Down Down ## [183] Up Up Down Up Down Up Down Down Down Down Down Up Down Up ## [197] Down Down Up Up Down Up Down Down Up Down Down Down Up Up ## [211] Up Up Up Up Down Down Up Up Up Up Down Up Up Up ## [225] Up Up Up Down Down Up Down Up Up Down Up Down Up Up ## [239] Up Up Up Down Down Down Up Down Up Up Down Up Down Down ## Levels: Down Up The output is a vector containing the predicted outcomes for the test data. We can generate the confusion matrix and the test error rate: table(knn_pred, Smarket_05$Direction) ## ## knn_pred Down Up ## Down 92 22 ## Up 19 119 data_frame( actual = Smarket_05$Direction, predict = knn_pred ) %&gt;% with(mean(actual != predict)) ## [1] 0.163 Repeat with \\(K=3\\) and compare performance: knn_pred &lt;- knn(train = Smarket_0104_x, test = Smarket_05_x, cl = Smarket_0104$Direction, k = 3) table(knn_pred, Smarket_05$Direction) ## ## knn_pred Down Up ## Down 92 21 ## Up 19 120 data_frame( actual = Smarket_05$Direction, predict = knn_pred ) %&gt;% with(mean(actual != predict)) ## [1] 0.159 4.6.6 An application to Caravan insurance data Let’s apply KNN to the Caravan data set from ISLR. The response variable is Purchase which indicates whether or not a given individual purchases a caravan insurance policy. Caravan &lt;- as_tibble(Caravan) Caravan ## # A tibble: 5,822 x 86 ## MOSTYPE MAANTHUI MGEMOMV MGEMLEEF MOSHOOFD MGODRK MGODPR MGODOV MGODGE ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 33 1 3 2 8 0 5 1 3 ## 2 37 1 2 2 8 1 4 1 4 ## 3 37 1 2 2 8 0 4 2 4 ## 4 9 1 3 3 3 2 3 2 4 ## 5 40 1 4 2 10 1 4 1 4 ## 6 23 1 2 1 5 0 5 0 5 ## # ... with 5,816 more rows, and 77 more variables: MRELGE &lt;dbl&gt;, ## # MRELSA &lt;dbl&gt;, MRELOV &lt;dbl&gt;, MFALLEEN &lt;dbl&gt;, MFGEKIND &lt;dbl&gt;, ## # MFWEKIND &lt;dbl&gt;, MOPLHOOG &lt;dbl&gt;, MOPLMIDD &lt;dbl&gt;, MOPLLAAG &lt;dbl&gt;, ## # MBERHOOG &lt;dbl&gt;, MBERZELF &lt;dbl&gt;, MBERBOER &lt;dbl&gt;, MBERMIDD &lt;dbl&gt;, ## # MBERARBG &lt;dbl&gt;, MBERARBO &lt;dbl&gt;, MSKA &lt;dbl&gt;, MSKB1 &lt;dbl&gt;, MSKB2 &lt;dbl&gt;, ## # MSKC &lt;dbl&gt;, MSKD &lt;dbl&gt;, MHHUUR &lt;dbl&gt;, MHKOOP &lt;dbl&gt;, MAUT1 &lt;dbl&gt;, ## # MAUT2 &lt;dbl&gt;, MAUT0 &lt;dbl&gt;, MZFONDS &lt;dbl&gt;, MZPART &lt;dbl&gt;, MINKM30 &lt;dbl&gt;, ## # MINK3045 &lt;dbl&gt;, MINK4575 &lt;dbl&gt;, MINK7512 &lt;dbl&gt;, MINK123M &lt;dbl&gt;, ## # MINKGEM &lt;dbl&gt;, MKOOPKLA &lt;dbl&gt;, PWAPART &lt;dbl&gt;, PWABEDR &lt;dbl&gt;, ## # PWALAND &lt;dbl&gt;, PPERSAUT &lt;dbl&gt;, PBESAUT &lt;dbl&gt;, PMOTSCO &lt;dbl&gt;, ## # PVRAAUT &lt;dbl&gt;, PAANHANG &lt;dbl&gt;, PTRACTOR &lt;dbl&gt;, PWERKT &lt;dbl&gt;, ## # PBROM &lt;dbl&gt;, PLEVEN &lt;dbl&gt;, PPERSONG &lt;dbl&gt;, PGEZONG &lt;dbl&gt;, ## # PWAOREG &lt;dbl&gt;, PBRAND &lt;dbl&gt;, PZEILPL &lt;dbl&gt;, PPLEZIER &lt;dbl&gt;, ## # PFIETS &lt;dbl&gt;, PINBOED &lt;dbl&gt;, PBYSTAND &lt;dbl&gt;, AWAPART &lt;dbl&gt;, ## # AWABEDR &lt;dbl&gt;, AWALAND &lt;dbl&gt;, APERSAUT &lt;dbl&gt;, ABESAUT &lt;dbl&gt;, ## # AMOTSCO &lt;dbl&gt;, AVRAAUT &lt;dbl&gt;, AAANHANG &lt;dbl&gt;, ATRACTOR &lt;dbl&gt;, ## # AWERKT &lt;dbl&gt;, ABROM &lt;dbl&gt;, ALEVEN &lt;dbl&gt;, APERSONG &lt;dbl&gt;, ## # AGEZONG &lt;dbl&gt;, AWAOREG &lt;dbl&gt;, ABRAND &lt;dbl&gt;, AZEILPL &lt;dbl&gt;, ## # APLEZIER &lt;dbl&gt;, AFIETS &lt;dbl&gt;, AINBOED &lt;dbl&gt;, ABYSTAND &lt;dbl&gt;, ## # Purchase &lt;fct&gt; Caravan %&gt;% count(Purchase) %&gt;% mutate(pct = n / sum(n)) ## # A tibble: 2 x 3 ## Purchase n pct ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 No 5474 0.940 ## 2 Yes 348 0.0598 Only approximately 6% of individuals in the dataset purchased a caravan insurance policy. To perform KNN, first we standardize the data set using the scale() function. scale() normalizes any vector/variable to mean 0 and standard deviation 1. To apply this standardization to each column in Caravan (except for the Purchase column), we use mutate_at() to apply the same mutation function to multiple columns. Caravan_scale &lt;- Caravan %&gt;% mutate_at(.vars = vars(-Purchase), .funs = funs(scale(.) %&gt;% as.vector)) # confirm the transformation worked Caravan_scale %&gt;% summarize_at(.vars = vars(-Purchase), .funs = funs(mean, sd)) %&gt;% glimpse() ## Observations: 1 ## Variables: 170 ## $ MOSTYPE_mean &lt;dbl&gt; -7.03e-17 ## $ MAANTHUI_mean &lt;dbl&gt; -1.47e-16 ## $ MGEMOMV_mean &lt;dbl&gt; -1.78e-16 ## $ MGEMLEEF_mean &lt;dbl&gt; 2.04e-16 ## $ MOSHOOFD_mean &lt;dbl&gt; -1.46e-17 ## $ MGODRK_mean &lt;dbl&gt; -4.68e-17 ## $ MGODPR_mean &lt;dbl&gt; 7.08e-17 ## $ MGODOV_mean &lt;dbl&gt; 1.08e-17 ## $ MGODGE_mean &lt;dbl&gt; 9.04e-17 ## $ MRELGE_mean &lt;dbl&gt; -1.35e-16 ## $ MRELSA_mean &lt;dbl&gt; -1.44e-17 ## $ MRELOV_mean &lt;dbl&gt; -1.2e-16 ## $ MFALLEEN_mean &lt;dbl&gt; -1.13e-18 ## $ MFGEKIND_mean &lt;dbl&gt; 4.74e-17 ## $ MFWEKIND_mean &lt;dbl&gt; -6.68e-17 ## $ MOPLHOOG_mean &lt;dbl&gt; -4.63e-17 ## $ MOPLMIDD_mean &lt;dbl&gt; 1.07e-16 ## $ MOPLLAAG_mean &lt;dbl&gt; 4.04e-17 ## $ MBERHOOG_mean &lt;dbl&gt; -2.21e-17 ## $ MBERZELF_mean &lt;dbl&gt; 1.14e-17 ## $ MBERBOER_mean &lt;dbl&gt; -3.95e-17 ## $ MBERMIDD_mean &lt;dbl&gt; -1.71e-17 ## $ MBERARBG_mean &lt;dbl&gt; 1.16e-16 ## $ MBERARBO_mean &lt;dbl&gt; -1.03e-16 ## $ MSKA_mean &lt;dbl&gt; -6.42e-17 ## $ MSKB1_mean &lt;dbl&gt; 5.22e-17 ## $ MSKB2_mean &lt;dbl&gt; -3.22e-18 ## $ MSKC_mean &lt;dbl&gt; 3.38e-17 ## $ MSKD_mean &lt;dbl&gt; 1.03e-16 ## $ MHHUUR_mean &lt;dbl&gt; 1.08e-17 ## $ MHKOOP_mean &lt;dbl&gt; -3.33e-17 ## $ MAUT1_mean &lt;dbl&gt; -2.71e-16 ## $ MAUT2_mean &lt;dbl&gt; 1.02e-16 ## $ MAUT0_mean &lt;dbl&gt; 1.76e-17 ## $ MZFONDS_mean &lt;dbl&gt; 4.33e-17 ## $ MZPART_mean &lt;dbl&gt; -3.26e-17 ## $ MINKM30_mean &lt;dbl&gt; -8.52e-17 ## $ MINK3045_mean &lt;dbl&gt; 4.2e-17 ## $ MINK4575_mean &lt;dbl&gt; -3.71e-17 ## $ MINK7512_mean &lt;dbl&gt; -2.19e-17 ## $ MINK123M_mean &lt;dbl&gt; -2.4e-17 ## $ MINKGEM_mean &lt;dbl&gt; 1.6e-16 ## $ MKOOPKLA_mean &lt;dbl&gt; -1.85e-16 ## $ PWAPART_mean &lt;dbl&gt; -3.31e-17 ## $ PWABEDR_mean &lt;dbl&gt; -2.36e-18 ## $ PWALAND_mean &lt;dbl&gt; -2.25e-17 ## $ PPERSAUT_mean &lt;dbl&gt; -1.57e-17 ## $ PBESAUT_mean &lt;dbl&gt; 2.32e-18 ## $ PMOTSCO_mean &lt;dbl&gt; -1.64e-18 ## $ PVRAAUT_mean &lt;dbl&gt; 5.54e-18 ## $ PAANHANG_mean &lt;dbl&gt; 5.31e-18 ## $ PTRACTOR_mean &lt;dbl&gt; 2.94e-17 ## $ PWERKT_mean &lt;dbl&gt; -4.79e-18 ## $ PBROM_mean &lt;dbl&gt; 2.24e-17 ## $ PLEVEN_mean &lt;dbl&gt; -1.39e-18 ## $ PPERSONG_mean &lt;dbl&gt; -7.14e-19 ## $ PGEZONG_mean &lt;dbl&gt; -4.44e-18 ## $ PWAOREG_mean &lt;dbl&gt; -4.16e-18 ## $ PBRAND_mean &lt;dbl&gt; 3.03e-17 ## $ PZEILPL_mean &lt;dbl&gt; 3.52e-19 ## $ PPLEZIER_mean &lt;dbl&gt; 1.48e-18 ## $ PFIETS_mean &lt;dbl&gt; 2.25e-17 ## $ PINBOED_mean &lt;dbl&gt; -1.32e-18 ## $ PBYSTAND_mean &lt;dbl&gt; -1.42e-17 ## $ AWAPART_mean &lt;dbl&gt; -3.76e-17 ## $ AWABEDR_mean &lt;dbl&gt; -8.27e-18 ## $ AWALAND_mean &lt;dbl&gt; 1.18e-17 ## $ APERSAUT_mean &lt;dbl&gt; 2.61e-17 ## $ ABESAUT_mean &lt;dbl&gt; -4.78e-18 ## $ AMOTSCO_mean &lt;dbl&gt; 2.48e-17 ## $ AVRAAUT_mean &lt;dbl&gt; 5.18e-18 ## $ AAANHANG_mean &lt;dbl&gt; 4.88e-18 ## $ ATRACTOR_mean &lt;dbl&gt; -9.36e-18 ## $ AWERKT_mean &lt;dbl&gt; -2.05e-18 ## $ ABROM_mean &lt;dbl&gt; 9.16e-18 ## $ ALEVEN_mean &lt;dbl&gt; -9.64e-18 ## $ APERSONG_mean &lt;dbl&gt; 7.86e-18 ## $ AGEZONG_mean &lt;dbl&gt; 1.1e-18 ## $ AWAOREG_mean &lt;dbl&gt; 2.2e-18 ## $ ABRAND_mean &lt;dbl&gt; -4.89e-17 ## $ AZEILPL_mean &lt;dbl&gt; 2.85e-18 ## $ APLEZIER_mean &lt;dbl&gt; -1.06e-18 ## $ AFIETS_mean &lt;dbl&gt; 7.78e-18 ## $ AINBOED_mean &lt;dbl&gt; 6.96e-18 ## $ ABYSTAND_mean &lt;dbl&gt; -9.91e-18 ## $ MOSTYPE_sd &lt;dbl&gt; 1 ## $ MAANTHUI_sd &lt;dbl&gt; 1 ## $ MGEMOMV_sd &lt;dbl&gt; 1 ## $ MGEMLEEF_sd &lt;dbl&gt; 1 ## $ MOSHOOFD_sd &lt;dbl&gt; 1 ## $ MGODRK_sd &lt;dbl&gt; 1 ## $ MGODPR_sd &lt;dbl&gt; 1 ## $ MGODOV_sd &lt;dbl&gt; 1 ## $ MGODGE_sd &lt;dbl&gt; 1 ## $ MRELGE_sd &lt;dbl&gt; 1 ## $ MRELSA_sd &lt;dbl&gt; 1 ## $ MRELOV_sd &lt;dbl&gt; 1 ## $ MFALLEEN_sd &lt;dbl&gt; 1 ## $ MFGEKIND_sd &lt;dbl&gt; 1 ## $ MFWEKIND_sd &lt;dbl&gt; 1 ## $ MOPLHOOG_sd &lt;dbl&gt; 1 ## $ MOPLMIDD_sd &lt;dbl&gt; 1 ## $ MOPLLAAG_sd &lt;dbl&gt; 1 ## $ MBERHOOG_sd &lt;dbl&gt; 1 ## $ MBERZELF_sd &lt;dbl&gt; 1 ## $ MBERBOER_sd &lt;dbl&gt; 1 ## $ MBERMIDD_sd &lt;dbl&gt; 1 ## $ MBERARBG_sd &lt;dbl&gt; 1 ## $ MBERARBO_sd &lt;dbl&gt; 1 ## $ MSKA_sd &lt;dbl&gt; 1 ## $ MSKB1_sd &lt;dbl&gt; 1 ## $ MSKB2_sd &lt;dbl&gt; 1 ## $ MSKC_sd &lt;dbl&gt; 1 ## $ MSKD_sd &lt;dbl&gt; 1 ## $ MHHUUR_sd &lt;dbl&gt; 1 ## $ MHKOOP_sd &lt;dbl&gt; 1 ## $ MAUT1_sd &lt;dbl&gt; 1 ## $ MAUT2_sd &lt;dbl&gt; 1 ## $ MAUT0_sd &lt;dbl&gt; 1 ## $ MZFONDS_sd &lt;dbl&gt; 1 ## $ MZPART_sd &lt;dbl&gt; 1 ## $ MINKM30_sd &lt;dbl&gt; 1 ## $ MINK3045_sd &lt;dbl&gt; 1 ## $ MINK4575_sd &lt;dbl&gt; 1 ## $ MINK7512_sd &lt;dbl&gt; 1 ## $ MINK123M_sd &lt;dbl&gt; 1 ## $ MINKGEM_sd &lt;dbl&gt; 1 ## $ MKOOPKLA_sd &lt;dbl&gt; 1 ## $ PWAPART_sd &lt;dbl&gt; 1 ## $ PWABEDR_sd &lt;dbl&gt; 1 ## $ PWALAND_sd &lt;dbl&gt; 1 ## $ PPERSAUT_sd &lt;dbl&gt; 1 ## $ PBESAUT_sd &lt;dbl&gt; 1 ## $ PMOTSCO_sd &lt;dbl&gt; 1 ## $ PVRAAUT_sd &lt;dbl&gt; 1 ## $ PAANHANG_sd &lt;dbl&gt; 1 ## $ PTRACTOR_sd &lt;dbl&gt; 1 ## $ PWERKT_sd &lt;dbl&gt; 1 ## $ PBROM_sd &lt;dbl&gt; 1 ## $ PLEVEN_sd &lt;dbl&gt; 1 ## $ PPERSONG_sd &lt;dbl&gt; 1 ## $ PGEZONG_sd &lt;dbl&gt; 1 ## $ PWAOREG_sd &lt;dbl&gt; 1 ## $ PBRAND_sd &lt;dbl&gt; 1 ## $ PZEILPL_sd &lt;dbl&gt; 1 ## $ PPLEZIER_sd &lt;dbl&gt; 1 ## $ PFIETS_sd &lt;dbl&gt; 1 ## $ PINBOED_sd &lt;dbl&gt; 1 ## $ PBYSTAND_sd &lt;dbl&gt; 1 ## $ AWAPART_sd &lt;dbl&gt; 1 ## $ AWABEDR_sd &lt;dbl&gt; 1 ## $ AWALAND_sd &lt;dbl&gt; 1 ## $ APERSAUT_sd &lt;dbl&gt; 1 ## $ ABESAUT_sd &lt;dbl&gt; 1 ## $ AMOTSCO_sd &lt;dbl&gt; 1 ## $ AVRAAUT_sd &lt;dbl&gt; 1 ## $ AAANHANG_sd &lt;dbl&gt; 1 ## $ ATRACTOR_sd &lt;dbl&gt; 1 ## $ AWERKT_sd &lt;dbl&gt; 1 ## $ ABROM_sd &lt;dbl&gt; 1 ## $ ALEVEN_sd &lt;dbl&gt; 1 ## $ APERSONG_sd &lt;dbl&gt; 1 ## $ AGEZONG_sd &lt;dbl&gt; 1 ## $ AWAOREG_sd &lt;dbl&gt; 1 ## $ ABRAND_sd &lt;dbl&gt; 1 ## $ AZEILPL_sd &lt;dbl&gt; 1 ## $ APLEZIER_sd &lt;dbl&gt; 1 ## $ AFIETS_sd &lt;dbl&gt; 1 ## $ AINBOED_sd &lt;dbl&gt; 1 ## $ ABYSTAND_sd &lt;dbl&gt; 1 We can now fit the KNN model. First we split the observations into a test set containing the first 1,000 observations, and a training set containing the remaining observations. Then we fit a KNN model using the training data and \\(K=1\\) and evaluate its performance on the test data. Caravan_test &lt;- slice(Caravan_scale, 1:1000) Caravan_train &lt;- slice(Caravan_scale, 1001:n()) Caravan_test_x &lt;- select(Caravan_test, -Purchase) Caravan_train_x &lt;- select(Caravan_train, -Purchase) set.seed(1) knn_pred &lt;- knn(train = Caravan_train_x, test = Caravan_test_x, cl = Caravan_train$Purchase, k = 1) mean(Caravan_test$Purchase != knn_pred) # test error rate ## [1] 0.118 mean(Caravan_test$Purchase != &quot;No&quot;) # null baseline ## [1] 0.059 Compared to predicting “No” for each individual, this model performs poorly. If we only look at those predicted to buy insurance, the model actually performs better: table(knn_pred, Caravan_test$Purchase) ## ## knn_pred No Yes ## No 873 50 ## Yes 68 9 mean(Caravan_test$Purchase[knn_pred == &quot;Yes&quot;] == knn_pred[knn_pred == &quot;Yes&quot;]) ## [1] 0.117 Among those predicted to purchase insurance, \\(11.69%\\) actually do purchase insurance. This rate improves using \\(K=3\\) and \\(K=5\\) knn_pred &lt;- knn(train = Caravan_train_x, test = Caravan_test_x, cl = Caravan_train$Purchase, k = 3) mean(Caravan_test$Purchase[knn_pred == &quot;Yes&quot;] == knn_pred[knn_pred == &quot;Yes&quot;]) ## [1] 0.192 knn_pred &lt;- knn(train = Caravan_train_x, test = Caravan_test_x, cl = Caravan_train$Purchase, k = 5) mean(Caravan_test$Purchase[knn_pred == &quot;Yes&quot;] == knn_pred[knn_pred == &quot;Yes&quot;]) ## [1] 0.267 We can compare the performance of KNN to a logistic regression model. By relaxing the threshold for predicting purchase of insurance from \\(0.5\\) to \\(0.25\\), our model’s test error rate improves even more than for the KNN model. glm_fit &lt;- glm(Purchase ~ ., data = Caravan_train, family = binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred augment(glm_fit, newdata = Caravan_test, type.predict = &quot;response&quot;) %&gt;% as_tibble() %&gt;% # generate prediction mutate(.predict = ifelse(.fitted &gt;= .25, &quot;Yes&quot;, &quot;No&quot;)) %&gt;% # only evaluate individuals predicted to purchase insurance filter(.predict == &quot;Yes&quot;) %&gt;% # calculate accuracy rate for this subset with(mean(Purchase == .predict)) ## [1] 0.333 4.7 Session information devtools::session_info() ## Session info ------------------------------------------------------------- ## setting value ## version R version 3.5.0 (2018-04-23) ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz America/Chicago ## date 2018-07-24 ## Packages ----------------------------------------------------------------- ## package * version date source ## backports 1.1.2 2017-12-13 CRAN (R 3.5.0) ## base * 3.5.0 2018-04-24 local ## bookdown 0.7 2018-02-18 CRAN (R 3.5.0) ## compiler 3.5.0 2018-04-24 local ## datasets * 3.5.0 2018-04-24 local ## devtools 1.13.5 2018-02-18 CRAN (R 3.5.0) ## digest 0.6.15 2018-01-28 CRAN (R 3.5.0) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.5.0) ## graphics * 3.5.0 2018-04-24 local ## grDevices * 3.5.0 2018-04-24 local ## htmltools 0.3.6 2017-04-28 CRAN (R 3.5.0) ## knitr 1.20 2018-02-20 CRAN (R 3.5.0) ## magrittr 1.5 2014-11-22 CRAN (R 3.5.0) ## memoise 1.1.0 2017-04-21 CRAN (R 3.5.0) ## methods * 3.5.0 2018-04-24 local ## Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) ## rmarkdown 1.9 2018-03-01 CRAN (R 3.5.0) ## rprojroot 1.3-2 2018-01-03 CRAN (R 3.5.0) ## stats * 3.5.0 2018-04-24 local ## stringi 1.2.2 2018-05-02 CRAN (R 3.5.0) ## stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) ## tools 3.5.0 2018-04-24 local ## utils * 3.5.0 2018-04-24 local ## withr 2.1.2 2018-03-15 CRAN (R 3.5.0) ## xfun 0.1 2018-01-22 CRAN (R 3.5.0) ## yaml 2.1.19 2018-05-01 CRAN (R 3.5.0) "]
]
